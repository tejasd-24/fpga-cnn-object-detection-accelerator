# NeuralVerilog â€” FPGA CNN Accelerator

A fully custom **3-layer Convolutional Neural Network accelerator** implemented in Verilog and deployed on a **PYNQ Z2 (Zynq-7020)** FPGA board. Performs real-time object detection via USB webcam at **18 FPS** â€” all convolution, ReLU, and pooling computed entirely in FPGA hardware at **~7ms per image**.

---

## âœ¨ Highlights

| Metric | Value |
|--------|-------|
| **Real-time FPS** | **18 FPS** (FPGA) vs 3.7 FPS (ARM C-optimized) |
| **FPGA inference time** | **6.8 ms** per image |
| **FPGA speedup vs ARM** | **~5Ã—** faster (vs optimized C on Cortex-A9) |
| **Classification accuracy** | **56.1%** across 6 classes (3.4Ã— random chance) |
| **Parallel compute cores** | 16 convolution cores running simultaneously |
| **On-chip storage** | All feature maps in BRAM â€” zero external memory access |
| **Input resolution** | 128 Ã— 128 grayscale |
| **Classes** | airplane, cat, zebra, bus, bicycle, donut |

## ğŸ—ï¸ Architecture

### What Runs on FPGA (Programmable Logic)
All compute-intensive CNN operations run entirely on the FPGA fabric:

```
Input Image (128Ã—128Ã—1)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FPGA ACCELERATOR (PL)               â”‚
â”‚                                             â”‚
â”‚  Layer 1: Conv3Ã—3 (1â†’16ch) â†’ ReLU â†’ Pool2  â”‚
â”‚           128Ã—128 â†’ 64Ã—64, 16 channels      â”‚
â”‚                                             â”‚
â”‚  Layer 2: Conv3Ã—3 (16â†’32ch) â†’ ReLU â†’ Pool2  â”‚
â”‚           64Ã—64 â†’ 32Ã—32, 32 channels        â”‚
â”‚                                             â”‚
â”‚  Layer 3: Conv3Ã—3 (32â†’64ch) â†’ ReLU â†’ Pool2  â”‚
â”‚           32Ã—32 â†’ 16Ã—16, 64 channels        â”‚
â”‚                                             â”‚
â”‚  16 parallel conv cores Ã— accumulator       â”‚
â”‚  All feature maps stored in on-chip BRAM    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼  64 Ã— 16 Ã— 16 feature maps (read via AXI-Lite)
    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ARM CPU (PS)                        â”‚
â”‚                                             â”‚
â”‚  Spatial bin pooling (4Ã—4 grid) â†’ 1024 feat â”‚
â”‚  Linear classifier â†’ 6-class softmax       â”‚
â”‚  CAM-based bounding box generation          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
  Prediction + Bounding Box + Annotated JPEG
```

### Hardware Pipeline Detail
Each of the 3 CNN layers executes this pipeline in hardware:

1. **Line Buffer** â€” Caches 3 rows for streaming 3Ã—3 convolution
2. **Sliding Window** â€” Extracts 3Ã—3 patches from line buffer output
3. **16Ã— Conv Cores** â€” Parallel signed 8-bit multiply-accumulate (MAC)
4. **Accumulator** â€” Accumulates partial sums across input channels (tiled)
5. **ReLU** â€” Clamps negative values to zero with configurable right-shift
6. **Max Pooling** â€” 2Ã—2 stride-2 spatial downsampling

The FSM (`layer_fsm.v`) orchestrates all 3 layers sequentially, managing weight loading, channel tiling, and feature BRAM read/write addressing.

## ğŸ“ Project Structure

```
fpga-cnn-object-detection-accelerator/
â”‚
â”œâ”€â”€ rtl/                                  # Verilog RTL source
â”‚   â”œâ”€â”€ core/                            # CNN datapath modules
â”‚   â”‚   â”œâ”€â”€ cnn_acc_top.v                # Top-level: datapath + instantiations
â”‚   â”‚   â”œâ”€â”€ layer_fsm.v                  # FSM: orchestrates 3 layers
â”‚   â”‚   â”œâ”€â”€ conv_core.v                  # 3Ã—3 signed convolution MAC
â”‚   â”‚   â”œâ”€â”€ accumulator.v               # Multi-channel partial sum accumulator
â”‚   â”‚   â”œâ”€â”€ ReLU.v                       # Rectified linear unit
â”‚   â”‚   â”œâ”€â”€ max_pooling_engine.v         # 2Ã—2 max pooling
â”‚   â”‚   â”œâ”€â”€ line_buffer.v               # 3-row line buffer for streaming
â”‚   â”‚   â”œâ”€â”€ sliding_window.v            # 3Ã—3 window extractor
â”‚   â”‚   â”œâ”€â”€ weight_bram.v               # Weight storage BRAM
â”‚   â”‚   â””â”€â”€ feature_bram.v              # Feature map storage BRAM
â”‚   â””â”€â”€ axi_wrapper/                     # AXI-Lite interface
â”‚       â”œâ”€â”€ lyr3_cnn_axi.v              # AXI top wrapper
â”‚       â””â”€â”€ lyr3_cnn_axi_slave_lite_v1_0_S00_AXI.v
â”‚
â”œâ”€â”€ fpga/hw/                             # Pre-built FPGA binaries
â”‚   â”œâ”€â”€ lyr3_cnn.bit                    # Bitstream for PYNQ Z2
â”‚   â””â”€â”€ lyr3_cnn.hwh                    # Hardware handoff file
â”‚
â”œâ”€â”€ sim/                                  # Simulation & testbenches
â”‚   â”œâ”€â”€ module/cc_sim.v                  # Conv core unit test
â”‚   â””â”€â”€ top/tb.v                         # Full-system testbench
â”‚
â”œâ”€â”€ software/                             # PYNQ deployment scripts
â”‚   â”œâ”€â”€ realtime_detect.py              # â˜… Real-time webcam detection (FPGA + ARM)
â”‚   â”œâ”€â”€ pynq_inference.py               # Single-image FPGA inference + visualization
â”‚   â”œâ”€â”€ arm_cnn.c                       # Optimized C CNN for ARM inference
â”‚   â”œâ”€â”€ arm_benchmark.py                # ARM-only speed benchmark
â”‚   â”œâ”€â”€ fast_readout.c                  # C-accelerated FPGA feature readout
â”‚   â”œâ”€â”€ dump_fpga_features.py           # Dump FPGA features for classifier training
â”‚   â”œâ”€â”€ dump_arm_features.py            # Dump ARM features for classifier training
â”‚   â””â”€â”€ retrain_classifier.py           # Train linear classifier on dumped features
â”‚
â”œâ”€â”€ training/                             # Model training (laptop/GPU)
â”‚   â””â”€â”€ train_cnn.py                    # PyTorch quantization-aware CNN training
â”‚
â”œâ”€â”€ weights/                              # Pre-trained model weights
â”‚   â”œâ”€â”€ weights.bin                     # Quantized CNN weights (23,184 bytes)
â”‚   â”œâ”€â”€ fc_weight.npy                   # Classifier weights (6Ã—1024)
â”‚   â”œâ”€â”€ fc_bias.npy                     # Classifier biases (6,)
â”‚   â””â”€â”€ classes.json                    # Class name mapping
â”‚
â”œâ”€â”€ Vivado_block_design/                  # Vivado block design screenshot
â”‚   â””â”€â”€ Vivado_bd.png
â”‚
â””â”€â”€ docs/                                 # Documentation & diagrams
```

## ğŸ”§ How It Works

### 1. Train the CNN (on laptop/GPU)
```bash
python3 training/train_cnn.py \
    --train-dir ~/coco/train2017 \
    --train-ann ~/coco/annotations/instances_train2017.json \
    --val-dir ~/coco/val2017 \
    --val-ann ~/coco/annotations/instances_val2017.json \
    --output-dir weights
```

This trains a quantization-aware 3-layer CNN in PyTorch, then exports:
- `weights.bin` â€” 8-bit quantized conv weights (23,184 bytes)
- Test images as 128Ã—128 raw `.bin` files

### 2. Deploy to FPGA
```bash
# Copy bitstream + weights to PYNQ
scp fpga/hw/lyr3_cnn.bit \
    fpga/hw/lyr3_cnn.hwh \
    weights/weights.bin \
    xilinx@<pynq-ip>:/home/xilinx/jupyter_notebooks/CNN/
```

### 3. Dump FPGA Features & Retrain Classifier
```bash
# On PYNQ â€” extract features from all test images through FPGA
python3 dump_fpga_features.py

# Retrain the linear classifier on actual FPGA features
python3 retrain_classifier.py --features fpga_features.npz
```

### 4. Run Inference
```bash
# Single image classification
python3 pynq_inference.py --weights fpga_weights/weights.bin \
    --image fpga_weights/test_image_200_class2.bin
```

### 5. Retrain Classifier for ARM Mode
```bash
# On PYNQ â€” dump ARM CNN features from test images
python3 dump_arm_features.py

# Retrain classifier on ARM features
python3 retrain_classifier.py --features arm_features.npz --prefix arm_
```

### 6. Real-Time Webcam Detection
```bash
# FPGA mode â€” 18 FPS, custom CNN in hardware
python3 realtime_detect.py --mode fpga --res 320x240

# ARM mode â€” 3.7 FPS, same CNN in optimized C
python3 realtime_detect.py --mode arm --res 320x240
```

Open `http://<pynq-ip>:5000` in a browser for the live MJPEG stream with detection overlay.

## ğŸ“Š Results

### Per-Class Accuracy
| Class | Accuracy | Samples |
|-------|----------|---------|
| Airplane | 73.2% | 97 |
| Zebra | 71.8% | 85 |
| Donut | 58.1% | 62 |
| Bus | 49.0% | 100 |
| Cat | 45.0% | 100 |
| Bicycle | 43.0% | 100 |
| **Overall** | **56.1%** | **544** |

### Real-Time Performance (USB Webcam)
| Mode | FPS | Inference | Method |
|------|-----|-----------|--------|
| **FPGA** | **18** | 7ms conv + 18ms read | Custom 3-layer CNN in hardware |
| **ARM (C-opt)** | **3.7** | 248ms | Same CNN, optimized C on Cortex-A9 |
| ARM (numpy) | 0.3 | 3300ms | Same CNN, pure numpy (baseline) |

**FPGA speedup: ~5Ã— vs optimized ARM C**

## ğŸ› ï¸ FPGA Resource Usage

- **Target:** Xilinx Zynq-7020 (PYNQ Z2)
- **Convolution cores:** 16 parallel (processes 16 output channels simultaneously)
- **Accumulator BRAM:** 16 Ã— 4096 Ã— 24-bit (tiled for BRAM efficiency)
- **Feature BRAMs:** 112 channels Ã— 4096 Ã— 8-bit
- **Weight BRAM:** 23,184 Ã— 8-bit
- **Clock:** 50 MHz

## ğŸ”¬ Proof of FPGA Acceleration

Everything below the classifier runs on FPGA hardware â€” here's the evidence:

1. **RTL modules** â€” `cnn_acc_top.v` instantiates `conv_core` (MAC), `accumulator`, `ReLU`, and `max_pooling_engine` in a `generate` block â€” 16 parallel instances
2. **FSM control** â€” `layer_fsm.v` (500+ lines) orchestrates weight loading, 3-layer processing, tiling, and drain cycles â€” all in Verilog state machine
3. **DMA data path** â€” Python sends weights and image pixels to FPGA via AXI DMA, never processes them on ARM
4. **Start/done protocol** â€” Python pulses a start bit, then polls the FPGA's done flag â€” the ARM is idle during convolution
5. **Feature readout** â€” Python reads results from FPGA BRAMs via AXI-Lite registers (`REG_OUTPUT_CH`, `REG_OUTPUT_ADDR`, `REG_OUTPUT_DATA`)
6. **Timing** â€” 6.8 ms for 3 conv layers is consistent with a 50 MHz hardware pipeline, far too fast for ARM-side convolution

The **only** ARM-side computation is the final classifier: spatial bin pooling â†’ linear layer â†’ softmax â†’ CAM bounding box.

## ğŸ“‹ Requirements

### Hardware
- PYNQ Z2 board (Zynq-7020)
- MicroSD card with PYNQ v2.x image
- Ethernet connection to host

### Software (Training â€” laptop)
- Python 3.8+
- PyTorch
- pycocotools
- COCO dataset (train2017 + val2017 + annotations)

### Software (Inference â€” PYNQ)
- Python 3 (pre-installed on PYNQ)
- NumPy, Pillow (pre-installed on PYNQ)
- pynq library (pre-installed on PYNQ)

## ğŸ“ Key Design Decisions

- **8-bit quantization** â€” Weights stored as signed int8, activations as unsigned uint8, matching FPGA's integer arithmetic
- **Tiled accumulation** â€” Accumulator BRAM reduced from 16K to 4K entries by processing input channels in tiles of 16
- **Spatial bin pooling** â€” 4Ã—4 grid pooling (1024 features) preserves spatial layout for better shape discrimination vs. global average pooling (64 features)
- **CAM-based localization** â€” Class Activation Maps with saturated channel filtering and percentile-based thresholding for bounding box generation
- **Class-balanced training** â€” Inverse-frequency loss weighting handles imbalanced class distribution (62-100 samples per class)

## ğŸ“œ License

This project is for educational and research purposes.

## ğŸ™ Acknowledgments

- [PYNQ](http://www.pynq.io/) framework for Zynq FPGA development
- [MS COCO](https://cocodataset.org/) dataset for training images
- Xilinx Vivado for FPGA synthesis and implementation
